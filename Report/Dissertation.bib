@misc{,
title = {{Terraform by HashiCorp}},
url = {https://www.terraform.io/},
urldate = {2019-06-15}
}
@misc{,
title = {{Chef Infra | Chef}},
url = {https://www.chef.io/products/chef-infra/},
urldate = {2019-06-16}
}
@misc{,
keywords = {The 1000 Genomes Project is an international resea},
title = {{1000 Genomes Project and AWS}},
url = {https://aws.amazon.com/1000genomes/ http://aws.amazon.com/1000genomes/},
urldate = {2019-07-29}
}
@inproceedings{Agarwal2012,
abstract = {Performant execution of data-parallel jobs needs good execution plans. Certain properties of the code, the data, and the interaction between them are crucial to generate these plans. Yet, these properties are difficult to estimate due to the highly distributed nature of these frameworks, the freedom that allows users to specify arbitrary code as operations on the data, and since jobs in modern clusters have evolved beyond single map and reduce phases to logical graphs of operations. Using fixed apriori estimates of these properties to choose execution plans, as modern systems do, leads to poor performance in several instances. We present RoPE, a first step towards re-optimizing data-parallel jobs. RoPE collects certain code and data properties by piggybacking on job execution. It adapts execution plans by feeding these properties to a query optimizer. We show how this improves the future invocations of the same (and similar) jobs and characterize the scenarios of benefit. Experiments on Bing's production clusters show up to 2× improvement across response time for production jobs at the 75th percentile while using 1.5× fewer resources.},
author = {Agarwal, Sameer and Kandula, Srikanth and Bruno, Nico and Wu, Ming-Chuan and Stoica, Ion and Zhou, Jingren},
booktitle = {NSDI'12 Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation},
file = {:cs/home/jb260/Downloads/nsdi12-final228.pdf:pdf},
pages = {281--294},
publisher = {USENIX},
title = {{Re-optimizing data-parallel computing}},
url = {https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final228.pdf},
year = {2012}
}
@inproceedings{Alipourfard2017,
abstract = {– Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choos-ing poorly can significantly degrade performance and in-crease the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challeng-ing to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that lever-ages Bayesian Optimization to build performance mod-els for various applications, and the models are just ac-curate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90{\%} chance to find opti-mal configurations, otherwise near-optimal, saving up to 75{\%} search cost compared to existing solutions.},
author = {Alipourfard, Omid and Liu, Hongqiang Harry and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming and University, Yale and {Harry Liu}, Hongqiang and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming},
booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alipourfard et al. - 2017 - CherryPick Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics.pdf:pdf},
isbn = {978-1-931971-37-9},
pages = {469--482},
title = {{CherryPick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics}},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/alipourfard https://dl.acm.org/citation.cfm?id=3154669},
year = {2017}
}
@article{Armbrust2009,
abstract = {Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about over- provisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or under- provisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hlarge scale, is unprecedented in the history of IT.},
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy H. and Konwinski, Andrew and Lee, Gunho and Patterson, David A. and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Armbrust et al. - 2009 - Above the Clouds A Berkeley View of Cloud Computing.pdf:pdf},
journal = {EECS Department, University of California, Berkeley},
month = {feb},
pages = {1--25},
title = {{Above the Clouds: A Berkeley View of Cloud Computing}},
url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html},
year = {2009}
}
@article{Berl2010,
author = {Berl, A. and Gelenbe, E. and {Di Girolamo}, M. and Giuliani, G. and {De Meer}, H. and Dang, M. Q. and Pentikousis, K.},
doi = {10.1093/comjnl/bxp080},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berl et al. - 2010 - Energy-Efficient Cloud Computing.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
month = {sep},
number = {7},
pages = {1045--1051},
publisher = {Narnia},
title = {{Energy-Efficient Cloud Computing}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxp080},
volume = {53},
year = {2010}
}
@article{Berriman2013,
abstract = {The current model of transferring data from data centres to desktops for analysis will soon be rendered impractical by the accelerating growth in the volume of science datasets. Processing will instead often take place on high-performance servers co-located with data. Evaluations of how new technologies such as cloud computing would support such a new distributed computing model are urgently needed. Cloud computing is a new way of purchasing computing and storage resources on demand through virtualization technologies. We report here the results of investigations of the applicability of commercial cloud computing to scientific computing, with an emphasis on astronomy, including investigations of what types of applications can be run cheaply and efficiently on the cloud, and an example of an application well suited to the cloud: processing a large dataset to create a new science product.},
author = {Berriman, G Bruce and Deelman, Ewa and Juve, Gideon and Rynge, Mats and V{\"{o}}ckler, Jens S.},
doi = {10.1098/rsta.2012.0066},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berriman et al. - Unknown - The application of cloud computing to scientific workflows a study of cost and performance.pdf:pdf},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Astronomy,Cloud computing,Computer architecture,Data management,Grids},
number = {1983},
title = {{The application of cloud computing to scientific workflows: A study of cost and performance}},
url = {http://dx.doi.org/10.1098/rsta.2012.0066},
volume = {371},
year = {2013}
}
@inproceedings{Bilal2017,
abstract = {{\textcopyright} 2017 IEEE. Video streaming is one of the most popular and highest bandwidth consumers within the Internet today. Cloud's elastic and pay-per-use model offers viable solution to varying demands of heterogeneous viewers for large-scale video providers. Video providers are heavily exploiting cloud's elastic nature to cater the scalability and heterogeneity of video steaming related tasks. For instance, Netflix moved its whole infrastructure to Amazon cloud, and Twitch, one of the largest game streaming providers is owned by Amazon and now using Amazon's cloud. Video representations refer to multiple copies of same video transcoded in multiple bitrates, such as 240, 360, 720, 1080 etc. Viewers with varying bandwidth capacities are served with matching representations based on the available bandwidth to minimize buffering time and latency. However, video transcoding is a computation and communication intensive task, therefore, not all of the live videos are transcoded to different representations. For instance, Twitch transcodes only the video streams of premium member (which have 500+ regular viewers). All of the non-premium channels are broadcasted in source stream. A fundamental question therefore is: which channels should be considered to be transcoded to multiple representations to minimize the overall cloud leased resources cost and bandwidth, and to maximize user satisfaction. In this paper, we seek answer to this question by analyzing the impact of multiple representations on cost (based on leasing cloud resources), bandwidth, and Quality of Experience (QoE, measured in terms of user satisfaction). We use Twitch workload traces captured in 2015, to conduct the experimentation, and use latest real-world broadband and representation data rate statistics from Akamai and YouTube Live, and cost from Amazon EC2 and CloudFront to validate our results. Our analysis reveals that using cloud's resources to transcode channels with more than 40 average viewers per hour with a data rate of 720p or higher, leads to low cost and bandwidth consumption, and higher QoE, as compared to streaming source video without multiple representations.},
author = {Bilal, Kashif and Erbad, Aiman},
booktitle = {Proceedings - 2017 IEEE International Conference on Cloud Engineering, IC2E 2017},
doi = {10.1109/IC2E.2017.20},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bilal, Erbad - 2017 - Impact of Multiple Video Representations in Live Streaming A Cost, Bandwidth, and QoE Analysis.pdf:pdf},
isbn = {9781509058174},
keywords = {Cloud,Live streaming,QoE,Video Representations,Video Transcoding},
pages = {88--94},
title = {{Impact of multiple video representations in live streaming: A cost, bandwidth, and QoE analysis}},
url = {https://ieeexplore.ieee.org/ielx7/7922463/7923761/07923791.pdf},
year = {2017}
}
@inproceedings{Borhani2014,
abstract = {{\textcopyright} 2014 IEEE.Approaching a comprehensive performance benchmark for on-line transaction processing (OLTP) applications in a cloud environment is a challenging task. Fundamental features of clouds, such as the pay-as-you-go pricing model and unknown underlying configuration of the system, are contrary to the basic assumptions of available benchmarks such as TPC-W or RUBiS. In this paper, we introduce a systematic performance benchmark approach for OLTP applications on public clouds that use virtual machines(VMs). We propose WPress benchmark, which is based on the widespread blogging software, WordPress, as a representative OLTP application and implement an open source workload generator. Furthermore, we utilize a CPU micro-benchmark to investigate CPU performance of cloud-based VMs in greater detail. Average response time and total VM cost are the performance metrics measured by WPress. We evaluate small and large instance types of three real-life cloud providers, Amazon EC2, Microsoft Azure and Rackspace cloud. Results imply that Rackspace cloud has better average response times and total VM cost on small instances. However, Microsoft Azure is preferable for large instance type.},
author = {Borhani, Amir Hossein and Leitner, Philipp and Lee, Bu Sung and Li, Xiaorong and Hung, Terence},
booktitle = {Proceedings . IEEE 18th international Enterprise Distributed object computing conference},
doi = {10.1109/EDOC.2014.23},
file = {:cs/home/jb260/Downloads/06972056.pdf:pdf},
issn = {15417719},
keywords = {Amazon EC2,Benchmarking,CPU Micro-Benchmark,Cloud Computing,Microsoft Azure,OLTP,Rackspace Cloud,Virtual Machine},
number = {December},
pages = {101--109},
title = {{WPress: An Application-Driven Performance Benchmark for Cloud-Based Virtual Machines}},
volume = {2014-Decem},
year = {2014}
}
@article{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599},
author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
eprint = {1012.2599},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, de Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Model.pdf:pdf},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {http://arxiv.org/abs/1012.2599},
year = {2010}
}
@article{Buyya2009,
abstract = {With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing 'Storage Clouds' for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Buyya, Rajkumar and Yeo, Chee Shin and Venugopal, Srikumar and Broberg, James and Brandic, Ivona},
doi = {10.1016/j.future.2008.12.001},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buyya et al. - 2009 - Cloud computing and emerging IT platforms Vision, hype, and reality for delivering computing as the 5th utility.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Cloud computing,Data Centers,Market-oriented resource allocation,Utility computing,Virtualization},
month = {jun},
number = {6},
pages = {599--616},
publisher = {North-Holland},
title = {{Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility}},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X08001957},
volume = {25},
year = {2009}
}
@article{Chase2017,
abstract = {A key benefit of Amazon EC2-style cloud computing service is the ability to instantiate a large number of virtual machines (VMs) on the fly during flash crowd events. Most existing research focuses on the policy decision such as when and where to start a VM for an application. In this paper, we study a different problem: how can the VMs and the applications inside be brought up as quickly as possible? This problem has not been solved satisfactorily in existing cloud services. We develop a fast start technique for cloud applications by restoring previously created VM snapshots of fully initialized application. We propose a set of optimizations, including working set estimation, demand prediction, and free page avoidance, that allow an application to start running with only partially loaded memory, yet without noticeable performance penalty during its subsequent execution. We implement our system, called Twinkle, in the Xen hypervisor and employ the two-dimensional page walks supported by the latest virtualization technology. We use the RUBiS and TPC-W benchmarks to evaluate its performance under flash crowd and failure over scenarios. The results indicate that Twinkle can provision VMs and restore the QoS significantly faster than the current approaches.},
author = {Chase, Jonathan and Niyato, Dusit},
doi = {10.1109/TSC.2015.2476812},
file = {:cs/home/jb260/Downloads/05710870.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Cloud computing,scenario tree reduction,sensitivity analysis,software defined networking,stochastic optimization},
month = {apr},
number = {3},
pages = {396--409},
title = {{Joint Optimization of Resource Provisioning in Cloud Computing}},
url = {http://ieeexplore.ieee.org/document/5710870/},
volume = {10},
year = {2017}
}
@misc{CommonCrawl2012,
abstract = {Common Crawl produces and maintains a repository of web crawl data that is openly accessible to everyone. The crawl currently covers 5 billion pages and the repository includes valuable metadata. The crawl data is stored by Amazon's S3 service, allowing it to be bulk downloaded as well as directly accessed for map-reduce processing in EC2. This makes wholesale extraction, transformation, and analysis of web data cheap and easy. Small startups or even individuals can now access high quality crawl data that was previously only available to large search engine corporations.},
author = {{Common Crawl}},
keywords = {crawler,dataset,web},
title = {{Common Crawl}},
url = {http://commoncrawl.org/},
urldate = {2019-07-29}
}
@inproceedings{Conley2015,
abstract = {Cloud computing providers have recently begun to offer high-performance virtualized flash storage and virtualized network I/O capabilities, which have the potential to increase application performance. Since users pay for only the re- sources they use, these new resources have the potential to lower overall cost. Yet achieving low cost requires choosing the right mixture of resources, which is only possible if their performance and scaling behavior is known. In this paper, we present a systematic measurement of re- cently introduced virtualized storage and network I/O within Amazon Web Services (AWS). Our experience shows that there are scaling limitations in clusters relying on these new features. As a result, provisioning for a large-scale cluster differs substantially from small-scale deployments. We de- scribe the implications of this observation for achieving ef- ficiency in large-scale cloud deployments. To confirm the value of our methodology, we deploy cost-efficient, high- performance sorting of 100 TB as a large-scale evaluation.},
author = {Conley, Michael and Vahdat, Amin and Porter, George},
doi = {10.1145/2806777.2806781},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Conley, Vahdat, Porter - 2015 - Achieving cost-efficient, data-intensive computing in the cloud.pdf:pdf},
isbn = {9781450336512},
keywords = {C4 [Performance of Systems]: Measurement technique,I/O performance,Measurement Keywords Cloud},
pages = {302--314},
title = {{Achieving cost-efficient, data-intensive computing in the cloud}},
url = {http://dx.doi.org/10.1145/2806777.2806781},
year = {2015}
}
@inproceedings{Davatz2017,
abstract = {{\textcopyright} 2017 IEEE. A challenging problem for users of Infrastructure-As-A-Service (IaaS) clouds is selecting cloud providers, regions, and instance types cost-optimally for a given desired service level. Issues such as hardware heterogeneity, contention, and virtual machine (VM) placement can result in considerably differing performance across supposedly equivalent cloud resources. Existing research on cloud benchmarking helps, but often the focus is on providing low-level microbenchmarks (e.g., CPU or network speed), which are hard to map to concrete business metrics of enterprise cloud applications, such as request throughput of a multi-Tier Web application. In this paper, we propose Okta, a general approach for fairly and comprehensively benchmarking the performance and cost of a multi-Tier Web application hosted in an IaaS cloud. We exemplify our approach for a case study based on the two-Tier AcmeAir application, which we evaluate for 11 real-life deployment configurations on Amazon EC2 and Google Compute Engine. Our results show that for this application, choosing compute-optimized instance types in the Web layer and small bursting instances for the database tier leads to the overall most cost-effective deployments. This result held true for both cloud providers. The least cost-effective configuration in our study provides only about 67{\%} of throughput per US dollar spent. Our case study can serve as a blueprint for future industrial or academic application benchmarking projects.},
author = {Davatz, Christian and Inzinger, Christian and Scheuner, Joel and Leitner, Philipp},
booktitle = {Proceedings - 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017},
doi = {10.1109/CCGRID.2017.12},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davatz et al. - 2017 - An Approach and Case Study of Cloud Instance Type Selection for Multi-tier Web Applications.pdf:pdf},
isbn = {9781509066100},
keywords = {Application benchmarks,Bnchmarking,Cloud computing,Web applications},
month = {may},
pages = {534--543},
publisher = {IEEE},
title = {{An Approach and Case Study of Cloud Instance Type Selection for Multi-Tier Web Applications}},
url = {http://ieeexplore.ieee.org/document/7973740/},
year = {2017}
}
@inproceedings{Ferguson2012b,
abstract = {Data processing frameworks such as MapReduce [8] and Dryad [11] are used today in business environments where customers expect guaranteed performance. To date, how- ever, these systems are not capable of providing guarantees on job latency because scheduling policies are based on fair- sharing, and operators seek high cluster use through statis- tical multiplexing and over-subscription. With Jockey, we provide latency SLOs for data parallel jobs written in SCOPE. Jockey precomputes statistics using a simulator that captures the job's complex internal dependencies, accurately and ef- ficiently predicting the remaining run time at different re- source allocations and in different stages of the job. Our con- trol policy monitors a job's performance, and dynamically ad- justs resource allocation in the shared cluster in order to max- imize the job's economic utility while minimizing its impact on the rest of the cluster. In our experiments in Microsoft's production Cosmos clusters, Jockey meets the specified job latency SLOs and responds to changes in cluster conditions.},
author = {Ferguson, Andrew D and Bodik, Peter and Kandula, Srikanth and Boutin, Eric and Fonseca, Rodrigo},
booktitle = {EuroSys '12 (Proceedings of the 7th ACM European Conference on Computer Systems)},
doi = {10.1145/2168836.2168847},
file = {:cs/home/jb260/Downloads/p99-ferguson.pdf:pdf},
isbn = {9781450312233},
keywords = {5,and microsoft,at facebook,business environments as part,data parallel,deadline,dryad,dy-,ing increasing use in,mapreduce,namic adaptation,of near-,real time production systems,scheduling,slo},
pages = {99--112},
title = {{Jockey: Guaranteed Job Latency in Data Parallel Clusters}},
year = {2012}
}
@book{Georgiev2012,
abstract = {SSL (Secure Sockets Layer) is the de facto standard for secure In-ternet communications. Security of SSL connections against an active network attacker depends on correctly validating public-key certificates presented when the connection is established. We demonstrate that SSL certificate validation is completely broken in many security-critical applications and libraries. Vulnerable software includes Amazon's EC2 Java library and all cloud clients based on it; Amazon's and PayPal's merchant SDKs responsible for transmitting payment details from e-commerce sites to payment gateways; integrated shopping carts such as osCommerce, ZenCart, Ubercart, and PrestaShop; AdMob code used by mobile websites; Chase mobile banking and several other Android apps and libraries; Java Web-services middleware-including Apache Axis, Axis 2, Codehaus XFire, and Pusher library for Android-and all applications employing this middleware. Any SSL connection from any of these programs is insecure against a man-in-the-middle attack. The root causes of these vulnerabilities are badly designed APIs of SSL implementations (such as JSSE, OpenSSL, and GnuTLS) and data-transport libraries (such as cURL) which present developers with a confusing array of settings and options. We analyze perils and pitfalls of SSL certificate validation in software based on these APIs and present our recommendations.},
author = {Georgiev, Martin and Iyengar, Subodh and Jana, Suman and Anubhai, Rishita and Boneh, Dan and Shmatikov, Vitaly},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Georgiev et al. - 2012 - The Most Dangerous Code in the World Validating SSL Certificates in Non-Browser Software.pdf:pdf},
isbn = {9781450316514},
keywords = {C20 [Computer-Communication Networks]: General-Sec,HTTPS,TLS,public-key certifi-cates,public-key infrastructure,security vulnerabilities},
title = {{The Most Dangerous Code in the World: Validating SSL Certificates in Non-Browser Software}},
year = {2012}
}
@article{Gkatzikis2013,
abstract = {Contemporary mobile devices generate heavy loads of computationally intensive tasks, which cannot be executed locally due to the limited processing and energy capabilities of each device. Cloud facilities enable mobile devices-clients to offload their tasks to remote cloud servers, giving birth to Mobile Cloud Computing (MCC). The challenge for the cloud is to minimize the task execution and data transfer time to the user, whose location changes due to mobility. However, providing quality of service guarantees is particularly challenging in the dynamic MCC environment, due to the time-varying bandwidth of the access links, the ever changing available processing capacity at each server and the timevarying data volume of each virtual machine. In this article, we advocate the need for novel cloud architectures and migration mechanisms that effectively bring the computing power of the cloud closer to the mobile user. We consider a cloud computing architecture that consists of a back-end cloud and a local cloud, which is attached to wireless access infrastructure (e.g. LTE base stations). We outline different classes of task migration policies, spanning fully uncoordinated ones, in which each user or server autonomously makes its migration decisions, up to the cloud-wide migration strategy of a cloud provider. We conclude with a discussion of open research problems in the area.},
author = {Gkatzikis, Lazaros and Koutsopoulos, Iordanis},
doi = {10.1109/MWC.2013.6549280},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gkatzikis, Koutsopoulos - 2013 - Migrate or not Exploiting dynamic task migration in mobile cloud computing systems.pdf:pdf},
issn = {15361284},
journal = {IEEE Wireless Communications},
number = {3},
pages = {24--32},
title = {{Migrate or not? Exploiting dynamic task migration in mobile cloud computing systems}},
volume = {20},
year = {2013}
}
@techreport{Intricately2019,
abstract = {The cloud market is undergoing tremendous change. As new players enter the space, established cloud providers are faced with an entirely new set of challenges: increasing competition, rising customer expectations, complex use cases, and shifting spending habits. So how do the top players in the cloud space fare? And where do opportunities exist in the cloud market? The 2019 Intricately Cloud Market Share report leverages real customer data to answer these questions and more: To learn more about our platform, head to intricately.com/learnmore intricately.com},
author = {Intricately},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - 2019 Intricately Cloud Market Share Report.pdf:pdf},
institution = {Intricately},
title = {{2019 Intricately Cloud Market Share Report}},
url = {https://content.intricately.com/hubfs/Cloud-Market-Share-Report.pdf},
year = {2019}
}
@article{Jones1998,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
doi = {10.1023/A:1008306431147},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Schonlau, Welch - 1998 - Efficient Global Optimization of Expensive Black-Box Functions.pdf:pdf},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
number = {4},
pages = {455--492},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1008306431147.pdf},
volume = {13},
year = {1998}
}
@article{JunXin2005a,
author = {{Jun Xin} and {Chia-Wen Lin} and {Ming-Ting Sun}},
doi = {10.1109/JPROC.2004.839620},
file = {:cs/home/jb260/Downloads/01369700.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = {jan},
number = {1},
pages = {84--97},
title = {{Digital Video Transcoding}},
url = {http://ieeexplore.ieee.org/document/1369700/},
volume = {93},
year = {2005}
}
@incollection{Klein2014,
abstract = {NumPy is the fundamental package for scientific computing with Python. It contains among other things: - a powerful N-dimensional array object - sophisticated (broadcasting) functions - tools for integrating C/C++ and Fortran code - useful linear algebra, Fourier transform, and random number capabilities Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. NumPy is licensed under the BSD license, enabling reuse with few restrictions.},
author = {Klein, Bernd and Klein, Bernd},
booktitle = {Einf{\"{u}}hrung in Python 3},
doi = {10.3139/9783446441514.031},
month = {oct},
pages = {377--398},
publisher = {Carl Hanser Verlag GmbH {\&} Co. KG},
title = {{NumPy}},
year = {2014}
}
@article{Laaber2019,
abstract = {Rigorous performance engineering traditionally assumes measuring on bare-metal environments to control for as many confounding factors as possible. Unfortunately, some researchers and practitioners might not have access, knowledge, or funds to operate dedicated performance-testing hardware, making public clouds an attractive alternative. However, shared public cloud environments are inherently unpredictable in terms of the system performance they provide. In this study, we explore the effects of cloud environments on the variability of performance test results and to what extent slowdowns can still be reliably detected even in a public cloud. We focus on software microbenchmarks as an example of performance tests and execute extensive experiments on three different well-known public cloud services (AWS, GCE, and Azure) using three different cloud instance types per service. We also compare the results to a hosted bare-metal offering from IBM Bluemix. In total, we gathered more than 4.5 million unique microbenchmarking data points from benchmarks written in Java and Go. We find that the variability of results differs substantially between benchmarks and instance types (by a coefficient of variation from 0.03{\%} to {\textgreater}{\{}$\backslash$thinspace{\}}100{\%}). However, executing test and control experiments on the same instances (in randomized order) allows us to detect slowdowns of 10{\%} or less with high confidence, using state-of-the-art statistical tests (i.e., Wilcoxon rank-sum and overlapping bootstrapped confidence intervals). Finally, our results indicate that Wilcoxon rank-sum manages to detect smaller slowdowns in cloud environments.},
author = {Laaber, Christoph and Scheuner, Joel and Leitner, Philipp},
doi = {10.1007/s10664-019-09681-1},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laaber, Scheuner, Leitner - 2019 - Software microbenchmarking in the cloud. How bad is it really.pdf:pdf},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Cloud,Microbenchmarking,Performance testing,Performance-regression detection},
month = {apr},
pages = {1--40},
publisher = {Springer US},
title = {{Software microbenchmarking in the cloud. How bad is it really?}},
url = {http://link.springer.com/10.1007/s10664-019-09681-1},
year = {2019}
}
@article{Leitner2014,
abstract = {Benchmarking the performance of public cloud providers is a common research topic. Previous research has already extensively evaluated the performance of different cloud platforms for different use cases, and under different constraints and experiment setups. In this paper, we present a principled, large-scale literature review to collect and codify existing research regarding the predictability of performance in public Infrastructure-as-a-Service (IaaS) clouds. We formulate 15 hypotheses relating to the nature of performance variations in IaaS systems, to the factors of influence of performance variations, and how to compare different instance types. In a second step, we conduct extensive real-life experimentation on Amazon EC2 and Google Compute Engine to empirically validate those hypotheses. At the time of our research, performance in EC2 was substantially less predictable than in GCE. Further, we show that hardware heterogeneity is in practice less prevalent than anticipated by earlier research, while multi-tenancy has a dramatic impact on performance and predictability.},
archivePrefix = {arXiv},
arxivId = {1411.2429},
author = {Leitner, Philipp and Cito, Juergen},
doi = {10.1145/2885497},
eprint = {1411.2429},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leitner, Cito - 2016 - Patterns in the Chaos—A Study of Performance Variation and Predictability in Public IaaS Clouds.pdf:pdf},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
keywords = {Infrastructure-as-a-service,benchmarking,public cloud},
month = {apr},
number = {3},
pages = {1--23},
publisher = {ACM},
title = {{Patterns in the Chaos - a Study of Performance Variation and Predictability in Public IaaS Clouds}},
url = {http://dl.acm.org/citation.cfm?doid=2926746.2885497 http://arxiv.org/abs/1411.2429},
volume = {16},
year = {2014}
}
@article{Letham2019,
abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
doi = {10.1214/18-BA1110},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Letham et al. - 2019 - Constrained Bayesian Optimization with Noisy Experiments.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Bayesian optimization,quasi-Monte Carlo methods,randomized experiments},
number = {2},
pages = {495--519},
title = {{Constrained Bayesian Optimization with Noisy Experiments}},
url = {https://doi.org/10.1214/18-BA1110},
volume = {14},
year = {2019}
}
@article{Lottarini2018,
abstract = {{\textcopyright} 2018 Association for Computing Machinery. This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-aservice workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark and Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wach-, Mark},
doi = {10.1145/3173162.3173207},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lottarini et al. - 2018 - vbench Benchmarking Video Transcoding in the Cloud.pdf:pdf},
isbn = {9781450349116},
journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS '18},
pages = {797--809},
title = {{vbench : Benchmarking Video Transcoding in the Cloud}},
year = {2018}
}
@misc{LouisColumbus2018,
author = {{Louis Columbus}},
booktitle = {Forbes},
title = {{83{\%} Of Enterprise Workloads Will Be In The Cloud By 2020}},
url = {https://www.forbes.com/sites/louiscolumbus/2018/01/07/83-of-enterprise-workloads-will-be-in-the-cloud-by-2020/{\#}384d89a86261},
urldate = {2019-07-29},
year = {2018}
}
@article{McKinney2015,
abstract = {Title: Pandas - Powerful Python Data Analysis Toolkit SubTitle: ; Volume: ; Serie: ; Edition: ; Authors: McKinney, Wes And Team, PyData Development ; Year: 2015 ; Pages: 1625 ; Editor: ; Publisher: ; ISBN: ; Keywords: Pandas; Analysis; Data; Toolkit; Powerful; Python ;},
author = {McKinney, Wes and Team, PyData Development},
journal = {Pandas - Powerful Python Data Analysis Toolkit},
keywords = {Analysis,Data,Pandas,Powerful,Python,Toolkit},
pages = {1625},
title = {{Pandas - Powerful Python Data Analysis Toolkit}},
year = {2015}
}
@techreport{Mell2011,
address = {Gaithersburg, MD},
author = {Mell, P M and Grance, T},
booktitle = {Special Publication (NIST SP)},
doi = {10.6028/NIST.SP.800-145},
file = {:cs/home/jb260/Downloads/nistspecialpublication800-145.pdf:pdf},
institution = {National Institute of Standards and Technology},
pages = {800--145},
title = {{The NIST definition of cloud computing}},
url = {https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf},
year = {2011}
}
@inproceedings{Mu2012,
abstract = {The increasing popularity of cloud storage services attracts large amounts of companies to store their data in cloud instead of building their own infrastructures. With large amounts of data stored in the cloud, it is expected to provide high availability and fine global access experiences. However, there are still major concerns of the availability of major cloud services, especially in a sparsely connected global network with complicated issues. In this paper, we introduce $\mu$LibCloud, a system based on Apache libCloud, aiming to improve the availability and global access experience of clouds, and to tolerate provider failures and outages. $\mu$LibCloud works as a library at client side, transparently spreading and collecting data smartly to/from different cloud providers through erasure code. In evaluation, we deployed the system into 7 major cloud providers and run a global benchmarks from 9 locations around the world. The results were compared to the original clouds and a content delivery network. We observed that $\mu$LibCloud achieved a higher and more uniformed read availability in most cases, with reasonable estimated extra costs. For example, the read latency of some original providers could be reduced by 50{\%}-70{\%} at different locations. {\textcopyright} 2012 IEEE.},
author = {Mu, Shuai and Chen, Kang and Gao, Pin and Ye, Feng and Wu, Yongwei and Zheng, Weimin},
booktitle = {Proceedings - IEEE/ACM International Workshop on Grid Computing},
doi = {10.1109/Grid.2012.28},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mu et al. - 2012 - $\mu$libCloud Providing high available and uniform accessing to multiple cloud storages.pdf:pdf},
isbn = {9780769548159},
issn = {15505510},
keywords = {Cloud storage,Erasure code,Global access,High availability},
pages = {201--208},
title = {{$\mu$libCloud: Providing high available and uniform accessing to multiple cloud storages}},
year = {2012}
}
@book{Nocedal2006,
abstract = {Summary: Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Nocedal, Jorge and Wright, S},
booktitle = {Advances in Industrial Control},
doi = {10.1007/978-0-387-40065-5},
eprint = {NIHMS150003},
isbn = {978-0-387-30303-1},
issn = {21931577},
keywords = {Calculus of Variations and Optimal Control,Computational Mathematics and Numerical Analysis.,Computer science Mathematics.,Control.,Mathematical optimization.,Mathematics.,Operations Research/Decision Theory.,Optimization.,Systems Theory,Systems theory.},
number = {9781447122234},
pages = {31--45},
pmid = {661941},
publisher = {Springer New York},
series = {Springer Series in Operations Research and Financial Engineering},
title = {{Numerical Optimization 2nd Ed}},
url = {http://esc-web.lib.cbs.dk/login?url=http://dx.doi.org/10.1007/978-0-387-40065-5 http://link.springer.com/10.1007/978-0-387-40065-5},
year = {2006}
}
@inproceedings{Palit2016,
abstract = {? 2016 IEEE.The popularity of online services has grown exponentially, spurring great interest in improving server hardware and software. However, conducting research on servers has traditionally been challenging due to the complexity of setting up representative server configurations and measuring their performance. Recent work has eased the effort of benchmarking servers by making benchmarking software and benchmarking instructions readily available to the research community. Unfortunately, the existing benchmarks are a black box; their users are expected to trust the design decisions made in the construction of these benchmarks with little justification and few cited sources. In this work, we have attempted to overcome this problem by building new server benchmarks for three popular network-intensive workloads: video streaming, web serving, and object caching. This paper documents the benchmark construction process, describes the software, and provides the resources we used to justify the design decisions that make our benchmarks representative for system-level studies.},
author = {Palit, Tapti and Shen, Yongming and Ferdman, Michael},
booktitle = {ISPASS 2016 - International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2016.7482080},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Palit, Shen, Ferdman - Unknown - Demystifying Cloud Benchmarking.pdf:pdf},
isbn = {9781509019526},
month = {apr},
pages = {122--132},
publisher = {IEEE},
title = {{Demystifying cloud benchmarking}},
url = {http://ieeexplore.ieee.org/document/7482080/},
year = {2016}
}
@article{Pallis2010,
author = {Pallis, George},
doi = {10.1109/MIC.2010.113},
file = {:cs/home/jb260/Downloads/05562494.pdf:pdf},
issn = {1089-7801},
journal = {IEEE Internet Computing},
month = {sep},
number = {5},
pages = {70--73},
title = {{Cloud Computing: The New Frontier of Internet Computing}},
url = {http://ieeexplore.ieee.org/document/5562494/},
volume = {14},
year = {2010}
}
@inproceedings{Pegus,
abstract = {In this paper, we present an open, flexible and realistic benchmarking platform named Video BenchLab to measure the performance of streaming media workloads. While Video BenchLab can be used with any existing media server, we provide a set of tools for researchers to experiment with their own platform and protocols. The components include a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming media workloads, a dataset of freely available video and a client runtime to replay videos in the native video players of real Web browsers such as Firefox, Chrome and Internet Explorer. We define simple metrics that are able to capture the quality of video playback and identify issues that can happen during video replay. Finally, we provide a Dashboard to manage experiments, collect results and perform analytics to compare performance between experiments. We present a series of experiments with Video BenchLab to illustrate how the video specific metrics can be used to measure the user perceived experience in real browsers when streaming videos. We also show Internet scale experiments by deploying clients in data centers distributed all over the globe. All the software, datasets, workloads and results used in this paper are made freely available on SourceForge for anyone to reuse and expand.},
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
booktitle = {Proceedings of the 6th ACM Multimedia Systems Conference on - MMSys '15},
doi = {10.1145/2713168.2723145},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pegus, Cecchet, Shenoy - Unknown - Video BenchLab An Open Platform for Realistic Benchmarking of Streaming Media Workloads.pdf:pdf},
isbn = {9781450333511},
keywords = {D 28 [Metrics]: Performance measures General Terms,D25 [Testing and Debugging]: Testing tools,Experimentation Keywords Benchmarking,Performance,Streaming,Video,Web browsers},
pages = {165--176},
title = {{Video BenchLab}},
url = {http://dx.doi.org/10.1145/2713168.2723145 http://dl.acm.org/citation.cfm?doid=2713168.2723145},
year = {2015}
}
@article{Power2018a,
abstract = {There are lots of big companies that would love to switch from their big legacy systems to avoid compromises in functionality, make them more agile, lower IT costs, and help them to become faster to market. This article describes how they can make the move.},
author = {Power, Brad},
doi = {10.1109/MCC.2018.032591613},
file = {:cs/home/jb260/Downloads/08383676.pdf:pdf},
issn = {23256095},
journal = {IEEE Cloud Computing},
keywords = {Cloud Computing,SaaS,system integration},
month = {may},
number = {3},
pages = {27--30},
title = {{Digital Transformation Through SaaS Multiclouds}},
url = {https://ieeexplore.ieee.org/document/8383676/},
volume = {5},
year = {2018}
}
@article{Power2018,
abstract = {Cloud computing is too often seen as a tactical way to reduce costs, when its most important benefit is as a strategic way to grow revenues. Such revenue growth can come about in a variety of ways, such as through faster innovation of new products, processes, and customer interactions; identifying more customers and closing more purchases; and improving customer relationships through more targeted offers and better service and experiences. Companies that clearly understand the relative magnitude of cost savings and revenue growth and orient themselves toward the latter will better exploit the cloud and related technologies such as big data, artificial intelligence (AI), the Internet of Things, and blockchain, and thus strengthen their competitive advantage and customer value.},
author = {Power, Brad and Weinman, Joe},
doi = {10.1109/MCC.2018.043221018},
file = {:cs/home/jb260/Downloads/08436080.pdf:pdf},
issn = {23256095},
journal = {IEEE Cloud Computing},
month = {jul},
number = {4},
pages = {89--94},
title = {{Revenue growth is the primary benefit of the cloud}},
url = {https://ieeexplore.ieee.org/document/8436080/},
volume = {5},
year = {2018}
}
@inproceedings{Pu2010,
abstract = {Server virtualization offers the ability to slice large, underutilized physical servers into smaller, parallel virtual machines (VMs), enabling diverse applications to run in isolated environments on a shared hardware platform. Effective management of virtualized cloud environments introduces new and unique challenges, such as efficient CPU scheduling for virtual machines, effective allocation of virtual machines to handle both CPU intensive and I/O intensive workloads. Although a fair number of research projects have dedicated to measuring, scheduling, and resource management of virtual machines, there still lacks of in-depth understanding of the performance factors that can impact the efficiency and effectiveness of resource multiplexing and resource scheduling among virtual machines. In this paper, we present our experimental study on the performance interference in parallel processing of CPU and network intensive workloads in the Xen Virtual Machine Monitors (VMMs). We conduct extensive experiments to measure the performance interference among VMs running network I/O workloads that are either CPU bound or network bound. Based on our experiments and observations, we conclude with four key findings that are critical to effective management of virtualized cloud environments for both cloud service providers and cloud consumers. First, running network-intensive workloads in isolated environments on a shared hardware platform can lead to high overheads due to extensive context switches and events in driver domain and VMM. Second, co-locating CPU-intensive workloads in isolated environments on a shared hardware platform can incur high CPU contention due to the demand for fast memory pages exchanges in I/O channel. Third, running CPU-intensive workloads and network-intensive workloads in conjunction incurs the least resource contention, delivering higher aggregate performance. Last but not the least, identifying factors that impact the total demand of the exchanged memo-$\backslash$n-$\backslash$nry pages is critical to the in-depth understanding of the interference overheads in I/O channel in the driver domain and VMM.},
author = {Pu, Xing and Liu, Ling and Mei, Yiduo and Sivathanu, Sankaran and Koh, Younggyun and Pu, Calton},
booktitle = {Proceedings - 2010 IEEE 3rd International Conference on Cloud Computing, CLOUD 2010},
doi = {10.1109/CLOUD.2010.65},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pu et al. - 2010 - Understanding performance interference of IO workload in virtualized cloud environments.pdf:pdf},
isbn = {9780769541303},
pages = {51--58},
title = {{Understanding performance interference of I/O workload in virtualized cloud environments}},
year = {2010}
}
@misc{RCoreTeam2018,
address = {Vienna, Austria},
author = {{R Core Team}},
publisher = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {https://www.r-project.org/},
year = {2018}
}
@misc{RStudioTeam2016,
address = {Boston, MA},
author = {{RStudio Team}},
publisher = {RStudio, Inc.},
title = {{RStudio: Integrated Development Environment for R}},
url = {http://www.rstudio.com},
year = {2016}
}
@inproceedings{Ruiz-Alvarez2011,
abstract = {We present a new, automated approach to selecting the cloud storage service that best matches each dataset of a given application. Our approach relies on a machine read- able description of the capabilities (features, performance, cost, etc.) of each storage system, which is processed to- gether with the users specified requirements. The result is an assignment of datasets to storage systems, that has mul- tiple advantages: the resulting match meets performance requirements and estimates cost; users express their stor- age needs using high-level concepts rather than reading the documentation from different cloud providers and manu- ally calculating or estimating a solution. Together with our storage capabilities XML schema we present different use cases for our system that evaluate the Amazon, Azure and local clouds under several scenarios: choosing cloud stor- age services for a new application, estimating cost savings by switching storage services, estimating the evolution over time of cost and performance and providing information in an Amazon EC2 to Eucalyptus migration. Our application is able to process each use case in under 70 ms; it is also possible to easily expand it to account for new features and data requirements.},
address = {San Jose, California, USA},
author = {Ruiz-Alvarez, Arkaitz and Humphrey, Marty},
booktitle = {Proceedings of the 2nd international workshop on Scientific cloud computing - ScienceCloud '11},
doi = {10.1145/1996109.1996117},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruiz-Alvarez, Humphrey - 2011 - An automated approach to cloud storage service selection.pdf:pdf},
isbn = {9781450306997},
keywords = {cloud computing,cloud storage services,matching algorithms},
pages = {39},
publisher = {ACM Press},
title = {{An automated approach to cloud storage service selection}},
url = {http://portal.acm.org/citation.cfm?doid=1996109.1996117},
year = {2011}
}
@inproceedings{Samreen,
abstract = {Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules, each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of the complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting or migrating instances. We employ machine learning techniques to develop an adaptive deployment policy, providing an optimal match between the customer demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.},
author = {Samreen, Faiza and Elkhatib, Yehia and Rowe, Matthew and Blair, Gordon S},
booktitle = {Proceedings of the NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium},
doi = {10.1109/NOMS.2016.7502858},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Samreen et al. - Unknown - Daleel Simplifying Cloud Instance Selection Using Machine Learning.pdf:pdf},
isbn = {9781509002238},
keywords = {Cloud computing,Machine learning},
pages = {557--563},
title = {{Daleel: Simplifying cloud instance selection using machine learning}},
url = {http://www.planforcloud.com/},
year = {2016}
}
@inproceedings{Scheuner2018,
abstract = {The continuing growth of the cloud computing market has led to an unprecedented diversity of cloud services. To support service selection, micro-benchmarks are commonly used to identify the best performing cloud service. However, it remains unclear how relevant these synthetic micro-benchmarks are for gaining insights into the performance of real-world applications. Therefore, this paper develops a cloud benchmarking methodology that uses micro-benchmarks to profile applications and subsequently predicts how an application performs on a wide range of cloud services. A study with a real cloud provider (Amazon EC2) has been conducted to quantitatively evaluate the estimation model with 38 metrics from 23 micro-benchmarks and 2 applications from different domains. The results reveal remarkably low variability in cloud service performance and show that selected micro-benchmarks can estimate the duration of a scientific computing application with a relative error of less than 10{\%} and the response time of a Web serving application with a relative error between 10{\%} and 20{\%}. In conclusion, this paper emphasizes the importance of cloud benchmarking by substantiating the suitability of micro-benchmarks for estimating application performance in comparison to common baselines but also highlights that only selected micro-benchmarks are relevant to estimate the performance of a particular application.},
author = {Scheuner, Joel and Leitner, Philipp},
booktitle = {IEEE International Conference on Cloud Computing, CLOUD},
doi = {10.1109/CLOUD.2018.00019},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheuner, Leitner - 2018 - Estimating Cloud Application Performance Based on Micro-Benchmark Profiling.pdf:pdf},
isbn = {9781538672358},
issn = {21596190},
keywords = {Application benchmark,Benchmarking,Cloud computing,Micro benchmark,Performance,Performance prediction,Web application},
month = {jul},
pages = {90--97},
publisher = {IEEE},
title = {{Estimating Cloud Application Performance Based on Micro-Benchmark Profiling}},
url = {https://ieeexplore.ieee.org/document/8457787/},
volume = {2018-July},
year = {2018}
}
@inproceedings{Scheuner2018a,
abstract = {Micro and application performance benchmarks are commonly used to guide cloud service selection. However, they are often considered in isolation in a hardly reproducible setup with a flawed execution strategy. This paper presents a new execution methodology that combines micro and application benchmarks into a benchmark suite called RMIT Combined, integrates this suite into an automated cloud benchmarking environment, and implements a repeat-able execution strategy. Additionally, a newly crafted Web serving benchmark called WPBench with three different load scenarios is contributed. A case study in the Amazon EC2 cloud demonstrates that choosing a cost-efficient instance type can deliver up to 40{\%} better performance with 40{\%} lower costs at the same time for the Web serving benchmark WPBench. Contrary to prior research, our findings reveal that network performance does not vary relevantly anymore. Our results also show that choosing a modern type of virtualization can improve disk utilization up to 10{\%} for I/O-heavy workloads. CCS CONCEPTS • Software and its engineering → Cloud computing; Software performance;},
author = {Scheuner, Joel and Leitner, Philipp},
doi = {10.1145/3185768.3186286},
file = {:cs/home/jb260/Downloads/p161-scheuner.pdf:pdf},
pages = {161--166},
title = {{A Cloud Benchmark Suite Combining Micro and Applications Benchmarks}},
year = {2018}
}
@inproceedings{Scheuner2015,
abstract = {To optimally deploy their applications, users of Infrastructure-as-a-Service clouds are required to evaluate the costs and performance of different combinations of cloud configurations to find out which combination provides the best service level for their specific application. Unfortunately, benchmarking cloud services is cumbersome and error-prone. In this paper, we propose an architecture and concrete implementation of a cloud benchmarking Web service, which fosters the definition of reusable and representative benchmarks. In distinction to existing work, our system is based on the notion of Infrastructure-as-Code, which is a state of the art concept to define IT infrastructure in a reproducible, well-defined, and testable way. We demonstrate our system based on an illustrative case study, in which we measure and compare the disk IO speeds of different instance and storage types in Amazon EC2.},
author = {Scheuner, Joel and Leitner, Philipp and Cito, Jurgen and Gall, Harald},
booktitle = {Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom},
doi = {10.1109/CloudCom.2014.98},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheuner et al. - 2015 - Cloud work bench - Infrastructure-as-code based cloud benchmarking.pdf:pdf},
issn = {23302186},
keywords = {Benchmarking,Cloud computing,IaC,IaaS,Virtual machines},
number = {February},
pages = {246--253},
title = {{Cloud work bench - Infrastructure-as-code based cloud benchmarking}},
volume = {2015-Febru},
year = {2015}
}
@article{Shahriari2016,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and {De Freitas}, Nando},
doi = {10.1109/JPROC.2015.2494218},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahriari et al. - 2016 - Taking the human out of the loop A review of Bayesian optimization.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {http://www.ibm.com/software/commerce/optimization/},
volume = {104},
year = {2016}
}
@inproceedings{Shastri2017,
abstract = {Cloud spot markets o?er virtual machines (VMs) for a dynamic price that is much lower than the ?xed price of on-demand VMs. In exchange, spot VMs expose applications to multiple forms of risk, including price risk, or the risk that a VM's price will increase rela- tive to others. Since spot prices vary continuously across hundreds of di?erent types of VMs, ?exible applications can mitigate price risk by moving to the VM that currently o?ers the lowest cost. To enable this ?exibility, we present HotSpot, a resource container that “hops” VMs—by dynamically selecting and self-migrating to new VMs—as spot prices change. HotSpot containers de?ne a migration policy that lowers cost by determining when to hop VMs based on the transaction costs (from vacating a VM early and brie?y double paying for it) and bene?ts (the expected cost savings). As a side ef- fect ofmigrating to minimize cost, HotSpot is also able to reduce the number of revocations without degrading performance. HotSpot is simple and transparent: since it operates at the systems-level on each host VM, users need only run an HotSpot-enabled VM image to use it. We implement a HotSpot prototype on EC2, and evaluate it using job traces from a production Google cluster. We then compare HotSpot to using on-demand VMs and spot VMs (with and without fault-tolerance) in EC2, and show that it is able to lower cost and reduce the number of revocations without degrading performance.},
address = {Santa Clara, CA, USA},
author = {Shastri, Supreeth and Irwin, David},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing - SoCC '17},
doi = {10.1145/3127479.3132017},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shastri, Irwin - 2017 - HotSpot.pdf:pdf},
isbn = {9781450350280},
keywords = {hopping,price risk,revocation,spot market,transient server},
pages = {493--505},
publisher = {ACM Press},
title = {{HotSpot}},
url = {http://dl.acm.org/citation.cfm?doid=3127479.3132017},
year = {2017}
}
@article{Snoek2012,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
eprint = {1206.2944},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - Unknown - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
journal = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2951--2959},
publisher = {Curran Associates Inc.},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {https://dl.acm.org/citation.cfm?id=2999464 http://arxiv.org/abs/1206.2944},
year = {2012}
}
@inproceedings{Sousa2017,
abstract = {Multi-cloud computing has been proposed as a way to reduce vendor dependence, comply with location regulations, and optimize reliability, performance and costs. Meanwhile, microservice architectures are becoming increasingly popular in cloud computing as they promote decomposing applications into small services that can be independently deployed and scaled, thus optimizing resources usage. However, setting up a multi-cloud environment to deploy a microservices-based application is still a very complex and time consuming task. Each microservice may require different functionality (e.g. software platforms, databases, monitoring and scalability tools) and have different location and redundancy requirements. Selection of cloud providers should take into account the individual requirements of each service, as well as the global requirements of reliability and scalability. Moreover, cloud providers can be very heterogeneous and offer disparate functionality, thus hindering comparison. In this paper we propose an automated approach for the selection and configuration of cloud providers for multi-cloud microservices-based applications. Our approach uses a domain specific language to describe the application's multi-cloud requirements and we provide a systematic method for obtaining proper configurations that comply with the application's requirements and the cloud providers' constraints.},
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
booktitle = {IEEE International Conference on Cloud Computing, CLOUD},
doi = {10.1109/CLOUD.2016.49},
file = {:cs/home/jb260/Downloads/07820288.pdf:pdf},
isbn = {9781509026197},
issn = {21596190},
keywords = {Cloud management,Microservices,Multi-cloud,Software product lines,Variability management},
month = {jun},
pages = {327--334},
publisher = {IEEE},
title = {{Automated setup of multi-cloud environments for microservices applications}},
url = {http://ieeexplore.ieee.org/document/7820288/},
year = {2017}
}
@misc{SynergyResearchGroup2017,
author = {{Synergy Research Group}},
title = {{The Leading Cloud Providers Continue to Run Away with the Market | Synergy Research Group}},
url = {https://www.srgresearch.com/articles/leading-cloud-providers-continue-run-away-market},
urldate = {2019-07-29},
year = {2017}
}
@inproceedings{Thai2015,
abstract = {{\textcopyright} 2014 IEEE. When orchestrating Web service workflows, the geographical placement of the orchestration engine (s) can greatly affect workflow performance. Data may have to be transferred across long geographical distances, which in turn increases execution time and degrades the overall performance of a workflow. In this paper, we present a framework that, given a DAG-based workflow specification, computes the optimal Amazon EC2 cloud regions to deploy the orchestration engines and execute a workflow. The framework incorporates a constraint model that solves the workflow deployment problem, which is generated using an automated constraint modelling system. The feasibility of the framework is evaluated by executing different sample workflows representative of scientific workloads. The experimental results indicate that the framework reduces the workflow execution time and provides a speed up of 1.3x-2.5x over centralised approaches.},
author = {Thai, Long and Barker, Adam and Varghese, Blesson and Akgun, Ozgur and Miguel, Ian},
booktitle = {Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom},
doi = {10.1109/CloudCom.2014.30},
file = {:cs/home/jb260/Downloads/07037766.pdf:pdf},
isbn = {978-1-4799-4093-6},
issn = {23302186},
keywords = {Cloud computing,Optimal deployment,Workflow engine,Workflow execution},
month = {dec},
number = {February},
pages = {811--816},
publisher = {IEEE},
title = {{Optimal deployment of geographically distributed workflow engines on the cloud}},
url = {http://ieeexplore.ieee.org/document/7037766/},
volume = {2015-Febru},
year = {2015}
}
@article{Varghese,
abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings - one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.},
author = {Varghese, Blesson and Akgun, Ozgur and Miguel, Ian and Thai, Long and Barker, Adam},
doi = {10.1109/TCC.2016.2603476},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Varghese et al. - Unknown - Cloud Benchmarking for Maximising Performance of Scientific Applications.pdf:pdf},
issn = {21687161},
journal = {IEEE Transactions on Cloud Computing},
keywords = {Cloud benchmark,benchmarking methodology,cloud performance,cloud ranking},
number = {1},
pages = {170--182},
title = {{Cloud Benchmarking for Maximising Performance of Scientific Applications}},
url = {http://sysbench.sourceforge.net/},
volume = {7},
year = {2019}
}
@inproceedings{Varghese2016,
abstract = {With the availability of a wide range of cloud Virtual Machines (VMs) it is difficult to determine which VMs can maximise the performance of an application. Benchmarking is commonly used to this end for capturing the performance of VMs. Most cloud benchmarking techniques are typically heavyweight - time consuming processes which have to benchmark the entire VM in order to obtain accurate benchmark data. Such benchmarks cannot be used in real-time on the cloud and incur extra costs even before an application is deployed. In this paper, we present lightweight cloud benchmarking techniques that execute quickly and can be used in near real-time on the cloud. The exploration of lightweight benchmarking techniques are facilitated by the development of DocLite - Docker Container-based Lightweight Benchmarking. DocLite is built on the Docker container technology which allows a user-defined portion (such as memory size and the number of CPU cores) of the VM to be benchmarked. DocLite operates in two modes, in the first mode, containers are used to benchmark a small portion of the VM to generate performance ranks. In the second mode, historic benchmark data is used along with the first mode as a hybrid to generate VM ranks. The generated ranks are evaluated against three scientific high-performance computing applications. The proposed techniques are up to 91 times faster than a heavyweight technique which benchmarks the entire VM. It is observed that the first mode can generate ranks with over 90{\%} and 86{\%} accuracy for sequential and parallel execution of an application. The hybrid mode improves the correlation slightly but the first mode is sufficient for benchmarking cloud VMs.},
author = {Varghese, Blesson and Subba, Lawan Thamsuhang and Thai, Long and Barker, Adam},
booktitle = {Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016},
doi = {10.1109/IC2E.2016.28},
file = {:cs/home/jb260/Downloads/07484184.pdf:pdf},
isbn = {9781509019618},
keywords = {Docker,cloud benchmarking,containers,hybrid benchmark,lightweight benchmark},
month = {apr},
pages = {192--201},
publisher = {IEEE},
title = {{Container-based cloud virtual machine benchmarking}},
url = {http://ieeexplore.ieee.org/document/7484184/},
year = {2016}
}
@inproceedings{Varghese2016a,
abstract = {Existing benchmarking methods are time consuming processes as they typically benchmark the entire Virtual Machine (VM) in order to generate accurate performance data, making them less suitable for real-time analytics. The research in this paper is aimed to surmount the above challenge by presenting DocLite - Docker Container-based Lightweight benchmarking tool. DocLite explores lightweight cloud benchmarking methods for rapidly executing benchmarks in near real-time. DocLite is built on the Docker container technology, which allows a user-defined memory size and number of CPU cores of the VM to be benchmarked. The tool incorporates two benchmarking methods - the first referred to as the native method employs containers to benchmark a small portion of the VM and generate performance ranks, and the second uses historic benchmark data along with the native method as a hybrid to generate VM ranks. The proposed methods are evaluated on three use-cases and are observed to be up to 91 times faster than benchmarking the entire VM. In both methods, small containers provide the same quality of rankings as a large container. The native method generates ranks with over 90{\%} and 86{\%} accuracy for sequential and parallel execution of an application compared against benchmarking the whole VM. The hybrid method did not improve the quality of the rankings significantly.},
author = {Varghese, Blesson and Subba, Lawan Thamsuhang and Thai, Long and Barker, Adam},
booktitle = {Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016},
doi = {10.1109/CCGrid.2016.14},
file = {:cs/home/jb260/Downloads/07515691.pdf:pdf},
isbn = {9781509024520},
keywords = {Docker,cloud benchmarking,containers,hybrid benchmark,lightweight benchmark},
month = {may},
pages = {213--222},
publisher = {IEEE},
title = {{DocLite: A Docker-Based Lightweight Cloud Benchmarking Tool}},
url = {http://ieeexplore.ieee.org/document/7515691/},
year = {2016}
}
@article{Venkataraman2016,
abstract = {Recent workload trends indicate rapid growth in the deployment of machine learning, genomics and scientific workloads on cloud computing infrastructure. However, efficiently running these applications on shared infrastructure is challenging and we find that choosing the right hardware configuration can significantly improve performance and cost. The key to address the above challenge is having the ability to predict performance of applications under various resource configurations so that we can automatically choose the optimal configuration. Our insight is that a number of jobs have predictable structure in terms of computation and communication. Thus we can build performance models based on the behavior of the job on small samples of data and then predict its performance on larger datasets and cluster sizes. To minimize the time and resources spent in building a model, we use optimal experiment design, a statistical technique that allows us to collect as few training points as required. We have built Ernest, a performance prediction framework for large scale analytics and our evaluation on Amazon EC2 using several workloads shows that our prediction error is low while having a training overhead of less than 5{\%} for long-running jobs.},
author = {Venkataraman, Shivaram and Yang, Zongheng and Franklin, Michael and Recht, Benjamin and Nsdi, Implementation},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Venkataraman et al. - 2016 - Ernest Efficient Performance Prediction for Large-Scale Advanced Analytics.pdf:pdf},
isbn = {9781931971294},
journal = {NSDI'16 Proceedings of the 13th USENIX conference on Networked Systems Design and Implementation},
pages = {363--378},
title = {{Ernest : Efficient Performance Prediction for Large-Scale Advanced Analytics}},
year = {2016}
}
@article{Weinman2015,
abstract = {Price wars among cloud service providers have received a lot of press, with providers lowering prices due to reductions in cost structure, but also in an effort to gain share, receive publicity, and signal their aggressiveness in the market to competitors. Cloud pricing models, such as reserved instances, sustained-use pricing, and spot instances, however, are also of interest. They're a means of competitive differentiation as well as creating value for both customers and providers.},
author = {Weinman, Joe},
doi = {10.1109/MCC.2015.3},
file = {:cs/home/jb260/Downloads/07091800.pdf:pdf},
issn = {23256095},
journal = {IEEE Cloud Computing},
keywords = {Intercloud,cloud,cloud economics,pricing models},
month = {jan},
number = {1},
pages = {10--13},
title = {{Cloud pricing and markets}},
url = {http://ieeexplore.ieee.org/document/7091800/},
volume = {2},
year = {2015}
}
@inproceedings{Yadwadkar2017,
abstract = {Users of cloud services are presented with a bewildering choice of VM types and the choice ofVM can have significant implications on performance and cost. In this paper we address the fundamental problem of accurately and economically choosing the best VM for a given workload and user goals. To address the problem of opti- mal VM selection, we present PARIS, a data-driven system that uses a novel hybrid offline and online data collection and modeling framework to provide accurate performance estimates with mini- mal data collection. PARIS is able to predict workload performance for different user-specified metrics, and resulting costs for a wide range ofVM types and workloads across multiple cloud providers. When compared to sophisticated baselines, including collaborative filtering and a linear interpolation model using measured workload performance on two VM types, PARIS produces significantly better estimates of performance. For instance, it reduces runtime predic- tion error by a factor of 4 for some workloads on both AWS and Azure. The increased accuracy translates into a 45{\%} reduction in user cost while maintaining performance.},
address = {Santa Clara, CA, USA},
author = {Yadwadkar, Neeraja J. and Hariharan, Bharath and Gonzalez, Joseph E. and Smith, Burton and Katz, Randy H.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing - SoCC '17},
doi = {10.1145/3127479.3131614},
file = {:cs/home/jb260/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yadwadkar et al. - 2017 - Selecting the ibesti VM across multiple public clouds.pdf:pdf},
isbn = {9781450350280},
keywords = {cloud computing,data-driven modeling,performance prediction,resource allocation},
pages = {452--465},
publisher = {ACM Press},
title = {{Selecting the best VM across multiple public clouds}},
url = {http://dl.acm.org/citation.cfm?doid=3127479.3131614},
year = {2017}
}
