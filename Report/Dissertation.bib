@techreport{Samreen,
abstract = {Decision making in cloud environments is quite challenging due to the diversity in service offerings and pricing models, especially considering that the cloud market is an incredibly fast moving one. In addition, there are no hard and fast rules; each customer has a specific set of constraints (e.g. budget) and application requirements (e.g. minimum computational resources). Machine learning can help address some of these complicated decisions by carrying out customer-specific analytics to determine the most suitable instance type(s) and the most opportune time for starting and/or migrating instances. In this paper, we employ machine learning techniques to develop an adaptive deployment policy tailored for each customer, providing an optimal match between their demands and the available cloud service offerings. We provide an experimental study based on extensive set of job executions over a major public cloud infrastructure.},
author = {Samreen, Faiza and Elkhatib, Yehia and Rowe, Matthew and Blair, Gordon S},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samreen et al. - Unknown - Daleel Simplifying Cloud Instance Selection Using Machine Learning.pdf:pdf},
title = {{Daleel: Simplifying Cloud Instance Selection Using Machine Learning}},
url = {http://www.planforcloud.com/}
}
@article{Venkataraman2016,
abstract = {Recent workload trends indicate rapid growth in the deployment of machine learning, genomics and scientific workloads on cloud computing infrastructure. However, efficiently running these applications on shared infrastructure is challenging and we find that choosing the right hardware configuration can significantly improve performance and cost. The key to address the above challenge is having the ability to predict performance of applications under various resource configurations so that we can automatically choose the optimal configuration. Our insight is that a number of jobs have predictable structure in terms of computation and communication. Thus we can build performance models based on the behavior of the job on small samples of data and then predict its performance on larger datasets and cluster sizes. To minimize the time and resources spent in building a model, we use optimal experiment design, a statistical technique that allows us to collect as few training points as required. We have built Ernest, a performance prediction framework for large scale analytics and our evaluation on Amazon EC2 using several workloads shows that our prediction error is low while having a training overhead of less than 5{\%} for long-running jobs.},
author = {Venkataraman, Shivaram and Yang, Zongheng and Franklin, Michael and Recht, Benjamin and Nsdi, Implementation},
file = {:C$\backslash$:/Users/jackb/Downloads/nsdi16-paper-venkataraman.pdf:pdf},
isbn = {9781931971294},
journal = {NSDI'16 Proceedings of the 13th USENIX conference on Networked Systems Design and Implementation},
pages = {363--378},
title = {{Ernest : Efficient Performance Prediction for Large-Scale Advanced Analytics}},
year = {2016}
}
@inproceedings{Thai2015,
abstract = {{\textcopyright} 2014 IEEE. When orchestrating Web service workflows, the geographical placement of the orchestration engine (s) can greatly affect workflow performance. Data may have to be transferred across long geographical distances, which in turn increases execution time and degrades the overall performance of a workflow. In this paper, we present a framework that, given a DAG-based workflow specification, computes the optimal Amazon EC2 cloud regions to deploy the orchestration engines and execute a workflow. The framework incorporates a constraint model that solves the workflow deployment problem, which is generated using an automated constraint modelling system. The feasibility of the framework is evaluated by executing different sample workflows representative of scientific workloads. The experimental results indicate that the framework reduces the workflow execution time and provides a speed up of 1.3x-2.5x over centralised approaches.},
author = {Thai, Long and Barker, Adam and Varghese, Blesson and Akgun, Ozgur and Miguel, Ian},
booktitle = {Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom},
doi = {10.1109/CloudCom.2014.30},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thai et al. - 2015 - Optimal deployment of geographically distributed workflow engines on the cloud.pdf:pdf},
isbn = {978-1-4799-4093-6},
issn = {23302186},
keywords = {Cloud computing,Optimal deployment,Workflow engine,Workflow execution},
month = {dec},
number = {February},
pages = {811--816},
publisher = {IEEE},
title = {{Optimal deployment of geographically distributed workflow engines on the cloud}},
url = {http://ieeexplore.ieee.org/document/7037766/},
volume = {2015-Febru},
year = {2015}
}
@inproceedings{Sousa2017,
abstract = {Multi-cloud computing has been proposed as a way to reduce vendor dependence, comply with location regulations, and optimize reliability, performance and costs. Meanwhile, microservice architectures are becoming increasingly popular in cloud computing as they promote decomposing applications into small services that can be independently deployed and scaled, thus optimizing resources usage. However, setting up a multi-cloud environment to deploy a microservices-based application is still a very complex and time consuming task. Each microservice may require different functionality (e.g. software platforms, databases, monitoring and scalability tools) and have different location and redundancy requirements. Selection of cloud providers should take into account the individual requirements of each service, as well as the global requirements of reliability and scalability. Moreover, cloud providers can be very heterogeneous and offer disparate functionality, thus hindering comparison. In this paper we propose an automated approach for the selection and configuration of cloud providers for multi-cloud microservices-based applications. Our approach uses a domain specific language to describe the application's multi-cloud requirements and we provide a systematic method for obtaining proper configurations that comply with the application's requirements and the cloud providers' constraints.},
author = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
booktitle = {IEEE International Conference on Cloud Computing, CLOUD},
doi = {10.1109/CLOUD.2016.49},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sousa, Rudametkin, Duchien - 2017 - Automated setup of multi-cloud environments for microservices applications.pdf:pdf},
isbn = {9781509026197},
issn = {21596190},
keywords = {Cloud management,Microservices,Multi-cloud,Software product lines,Variability management},
month = {jun},
pages = {327--334},
publisher = {IEEE},
title = {{Automated setup of multi-cloud environments for microservices applications}},
url = {http://ieeexplore.ieee.org/document/7820288/},
year = {2017}
}
@book{Nocedal2006,
abstract = {Summary: Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Nocedal, Jorge and Wright, S},
booktitle = {Advances in Industrial Control},
doi = {10.1007/978-0-387-40065-5},
eprint = {NIHMS150003},
isbn = {978-0-387-30303-1},
issn = {21931577},
keywords = {Calculus of Variations and Optimal Control,Computational Mathematics and Numerical Analysis.,Computer science Mathematics.,Control.,Mathematical optimization.,Mathematics.,Operations Research/Decision Theory.,Optimization.,Systems Theory,Systems theory.},
number = {9781447122234},
pages = {31--45},
pmid = {661941},
publisher = {Springer New York},
series = {Springer Series in Operations Research and Financial Engineering},
title = {{Numerical Optimization 2nd Ed}},
url = {http://esc-web.lib.cbs.dk/login?url=http://dx.doi.org/10.1007/978-0-387-40065-5 http://link.springer.com/10.1007/978-0-387-40065-5},
year = {2006}
}
@inproceedings{Ferguson2012b,
abstract = {Data processing frameworks such as MapReduce [8] and Dryad [11] are used today in business environments where customers expect guaranteed performance. To date, how- ever, these systems are not capable of providing guarantees on job latency because scheduling policies are based on fair- sharing, and operators seek high cluster use through statis- tical multiplexing and over-subscription. With Jockey, we provide latency SLOs for data parallel jobs written in SCOPE. Jockey precomputes statistics using a simulator that captures the job's complex internal dependencies, accurately and ef- ficiently predicting the remaining run time at different re- source allocations and in different stages of the job. Our con- trol policy monitors a job's performance, and dynamically ad- justs resource allocation in the shared cluster in order to max- imize the job's economic utility while minimizing its impact on the rest of the cluster. In our experiments in Microsoft's production Cosmos clusters, Jockey meets the specified job latency SLOs and responds to changes in cluster conditions.},
author = {Ferguson, Andrew D and Bodik, Peter and Kandula, Srikanth and Boutin, Eric and Fonseca, Rodrigo},
booktitle = {EuroSys '12 (Proceedings of the 7th ACM European Conference on Computer Systems)},
doi = {10.1145/2168836.2168847},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferguson et al. - 2012 - Jockey.pdf:pdf},
isbn = {9781450312233},
keywords = {5,and microsoft,at facebook,business environments as part,data parallel,deadline,dryad,dy-,ing increasing use in,mapreduce,namic adaptation,of near-,real time production systems,scheduling,slo},
pages = {99--112},
title = {{Jockey: Guaranteed Job Latency in Data Parallel Clusters}},
year = {2012}
}
@article{Pegus,
abstract = {In this paper, we present an open, flexible and realistic benchmarking platform named Video BenchLab to measure the performance of streaming media workloads. While Video BenchLab can be used with any existing media server, we provide a set of tools for researchers to experiment with their own platform and protocols. The components include a MediaDrop video server, a suite of tools to bulk insert videos and generate streaming media workloads, a dataset of freely available video and a client runtime to replay videos in the native video players of real Web browsers such as Firefox, Chrome and Internet Explorer. We define simple metrics that are able to capture the quality of video playback and identify issues that can happen during video replay. Finally, we provide a Dashboard to manage experiments, collect results and perform analytics to compare performance between experiments. We present a series of experiments with Video BenchLab to illustrate how the video specific metrics can be used to measure the user perceived experience in real browsers when streaming videos. We also show Internet scale experiments by deploying clients in data centers distributed all over the globe. All the software, datasets, workloads and results used in this paper are made freely available on SourceForge for anyone to reuse and expand.},
author = {Pegus, Patrick and Cecchet, Emmanuel and Shenoy, Prashant},
doi = {10.1145/2713168.2723145},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pegus, Cecchet, Shenoy - Unknown - Video BenchLab An Open Platform for Realistic Benchmarking of Streaming Media Workloads.pdf:pdf},
isbn = {9781450333511},
keywords = {D 28 [Metrics]: Performance measures General Terms,D25 [Testing and Debugging]: Testing tools,Experimentation Keywords Benchmarking,Performance,Streaming,Video,Web browsers},
title = {{Video BenchLab: An Open Platform for Realistic Benchmarking of Streaming Media Workloads}},
url = {http://dx.doi.org/10.1145/2713168.2723145}
}
@misc{,
title = {{Terraform by HashiCorp}},
url = {https://www.terraform.io/},
urldate = {2019-06-15}
}
@misc{,
title = {{Chef Infra | Chef}},
url = {https://www.chef.io/products/chef-infra/},
urldate = {2019-06-16}
}
@inproceedings{Shastri2017,
abstract = {Cloud spot markets o?er virtual machines (VMs) for a dynamic price that is much lower than the ?xed price of on-demand VMs. In exchange, spot VMs expose applications to multiple forms of risk, including price risk, or the risk that a VM's price will increase rela- tive to others. Since spot prices vary continuously across hundreds of di?erent types of VMs, ?exible applications can mitigate price risk by moving to the VM that currently o?ers the lowest cost. To enable this ?exibility, we present HotSpot, a resource container that “hops” VMs—by dynamically selecting and self-migrating to new VMs—as spot prices change. HotSpot containers de?ne a migration policy that lowers cost by determining when to hop VMs based on the transaction costs (from vacating a VM early and brie?y double paying for it) and bene?ts (the expected cost savings). As a side ef- fect ofmigrating to minimize cost, HotSpot is also able to reduce the number of revocations without degrading performance. HotSpot is simple and transparent: since it operates at the systems-level on each host VM, users need only run an HotSpot-enabled VM image to use it. We implement a HotSpot prototype on EC2, and evaluate it using job traces from a production Google cluster. We then compare HotSpot to using on-demand VMs and spot VMs (with and without fault-tolerance) in EC2, and show that it is able to lower cost and reduce the number of revocations without degrading performance.},
address = {Santa Clara, CA, USA},
author = {Shastri, Supreeth and Irwin, David},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing - SoCC '17},
doi = {10.1145/3127479.3132017},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shastri, Irwin - 2017 - HotSpot(2).pdf:pdf},
isbn = {9781450350280},
keywords = {hopping,price risk,revocation,spot market,transient server},
pages = {493--505},
publisher = {ACM Press},
title = {{HotSpot}},
url = {http://dl.acm.org/citation.cfm?doid=3127479.3132017},
year = {2017}
}
@article{Varghese,
abstract = {How can applications be deployed on the cloud to achieve maximum performance? This question is challenging to address with the availability of a wide variety of cloud Virtual Machines (VMs) with different performance capabilities. The research reported in this paper addresses the above question by proposing a six step benchmarking methodology in which a user provides a set of weights that indicate how important memory, local communication, computation and storage related operations are to an application. The user can either provide a set of four abstract weights or eight fine grain weights based on the knowledge of the application. The weights along with benchmarking data collected from the cloud are used to generate a set of two rankings-one based only on the performance of the VMs and the other takes both performance and costs into account. The rankings are validated on three case study applications using two validation techniques. The case studies on a set of experimental VMs highlight that maximum performance can be achieved by the three top ranked VMs and maximum performance in a cost-effective manner is achieved by at least one of the top three ranked VMs produced by the methodology.},
author = {Varghese, Blesson and Akgun, Ozgur and Miguel, Ian and Thai, Long and Barker, Adam},
doi = {10.1109/TCC.2016.2603476},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varghese et al. - Unknown - Cloud Benchmarking for Maximising Performance of Scientific Applications.pdf:pdf},
keywords = {Index Terms-Cloud benchmark,benchmarking methodology,cloud performance,cloud ranking},
title = {{Cloud Benchmarking for Maximising Performance of Scientific Applications}},
url = {http://sysbench.sourceforge.net/}
}
@inproceedings{Yadwadkar2017,
abstract = {Users of cloud services are presented with a bewildering choice of VM types and the choice ofVM can have significant implications on performance and cost. In this paper we address the fundamental problem of accurately and economically choosing the best VM for a given workload and user goals. To address the problem of opti- mal VM selection, we present PARIS, a data-driven system that uses a novel hybrid offline and online data collection and modeling framework to provide accurate performance estimates with mini- mal data collection. PARIS is able to predict workload performance for different user-specified metrics, and resulting costs for a wide range ofVM types and workloads across multiple cloud providers. When compared to sophisticated baselines, including collaborative filtering and a linear interpolation model using measured workload performance on two VM types, PARIS produces significantly better estimates of performance. For instance, it reduces runtime predic- tion error by a factor of 4 for some workloads on both AWS and Azure. The increased accuracy translates into a 45{\%} reduction in user cost while maintaining performance.},
address = {Santa Clara, CA, USA},
author = {Yadwadkar, Neeraja J. and Hariharan, Bharath and Gonzalez, Joseph E. and Smith, Burton and Katz, Randy H.},
booktitle = {Proceedings of the 2017 Symposium on Cloud Computing - SoCC '17},
doi = {10.1145/3127479.3131614},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yadwadkar et al. - 2017 - Selecting the ibesti VM across multiple public clouds.pdf:pdf},
isbn = {9781450350280},
keywords = {cloud computing,data-driven modeling,performance prediction,resource allocation},
pages = {452--465},
publisher = {ACM Press},
title = {{Selecting the best VM across multiple public clouds}},
url = {http://dl.acm.org/citation.cfm?doid=3127479.3131614},
year = {2017}
}
@article{Lottarini2018,
abstract = {{\textcopyright} 2018 Association for Computing Machinery. This paper presents vbench, a publicly available benchmark for cloud video services. We are the first study, to the best of our knowledge, to characterize the emerging video-as-aservice workload. Unlike prior video processing benchmarks, vbench's videos are algorithmically selected to represent a large commercial corpus of millions of videos. Reflecting the complex infrastructure that processes and hosts these videos, vbench includes carefully constructed metrics and baselines. The combination of validated corpus, baselines, and metrics reveal nuanced tradeoffs between speed, quality, and compression. We demonstrate the importance of video selection with a microarchitectural study of cache, branch, and SIMD behavior. vbench reveals trends from the commercial corpus that are not visible in other video corpuses. Our experiments with GPUs under vbench's scoring scenarios reveal that context is critical: GPUs are well suited for live-streaming, while for video-on-demand shift costs from compute to storage and network. Counterintuitively, they are not viable for popular videos, for which highly compressed, high quality copies are required. We instead find that popular videos are currently well-served by the current trajectory of software encoders.},
author = {Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wachsler, Mark and Lottarini, Andrea and Ramirez, Alex and Coburn, Joel and Kim, Martha A and Ranganathan, Parthasarathy and Stodolsky, Daniel and Wach-, Mark},
doi = {10.1145/3173162.3173207},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lottarini et al. - 2018 - vbench Benchmarking Video Transcoding in the Cloud.pdf:pdf},
isbn = {9781450349116},
journal = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS '18},
pages = {797--809},
title = {{vbench : Benchmarking Video Transcoding in the Cloud}},
year = {2018}
}
@inproceedings{Alipourfard2017,
abstract = {– Picking the right cloud configuration for recurring big data analytics jobs running in clouds is hard, because there can be tens of possible VM instance types and even more cluster sizes to pick from. Choos-ing poorly can significantly degrade performance and in-crease the cost to run a job by 2-3x on average, and as much as 12x in the worst-case. However, it is challeng-ing to automatically identify the best configuration for a broad spectrum of applications and cloud configurations with low search cost. CherryPick is a system that lever-ages Bayesian Optimization to build performance mod-els for various applications, and the models are just ac-curate enough to distinguish the best or close-to-the-best configuration from the rest with only a few test runs. Our experiments on five analytic applications in AWS EC2 show that CherryPick has a 45-90{\%} chance to find opti-mal configurations, otherwise near-optimal, saving up to 75{\%} search cost compared to existing solutions.},
author = {Alipourfard, Omid and Liu, Hongqiang Harry and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming and University, Yale and {Harry Liu}, Hongqiang and Chen, Jianshu and Venkataraman, Shivaram and Yu, Minlan and Zhang, Ming},
booktitle = {Proceedings of the 14th USENIX Conference on Networked Systems Design and Implementation},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alipourfard et al. - 2017 - CherryPick Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics.pdf:pdf},
isbn = {978-1-931971-37-9},
pages = {469--482},
title = {{CherryPick: Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics}},
url = {https://www.usenix.org/conference/nsdi17/technical-sessions/presentation/alipourfard https://dl.acm.org/citation.cfm?id=3154669},
year = {2017}
}
@inproceedings{Palit2016,
abstract = {? 2016 IEEE.The popularity of online services has grown exponentially, spurring great interest in improving server hardware and software. However, conducting research on servers has traditionally been challenging due to the complexity of setting up representative server configurations and measuring their performance. Recent work has eased the effort of benchmarking servers by making benchmarking software and benchmarking instructions readily available to the research community. Unfortunately, the existing benchmarks are a black box; their users are expected to trust the design decisions made in the construction of these benchmarks with little justification and few cited sources. In this work, we have attempted to overcome this problem by building new server benchmarks for three popular network-intensive workloads: video streaming, web serving, and object caching. This paper documents the benchmark construction process, describes the software, and provides the resources we used to justify the design decisions that make our benchmarks representative for system-level studies.},
author = {Palit, Tapti and Shen, Yongming and Ferdman, Michael},
booktitle = {ISPASS 2016 - International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2016.7482080},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palit, Shen, Ferdman - Unknown - Demystifying Cloud Benchmarking.pdf:pdf},
isbn = {9781509019526},
month = {apr},
pages = {122--132},
publisher = {IEEE},
title = {{Demystifying cloud benchmarking}},
url = {http://ieeexplore.ieee.org/document/7482080/},
year = {2016}
}
@article{JunXin2005,
author = {{Jun Xin} and {Chia-Wen Lin} and {Ming-Ting Sun}},
doi = {10.1109/JPROC.2004.839620},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jun Xin, Chia-Wen Lin, Ming-Ting Sun - 2005 - Digital Video Transcoding.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = {jan},
number = {1},
pages = {84--97},
title = {{Digital Video Transcoding}},
url = {http://ieeexplore.ieee.org/document/1369700/},
volume = {93},
year = {2005}
}
@article{Snoek2012,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
eprint = {1206.2944},
file = {:C$\backslash$:/Users/jackb/Downloads/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf:pdf},
journal = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2951--2959},
publisher = {Curran Associates Inc.},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
url = {https://dl.acm.org/citation.cfm?id=2999464 http://arxiv.org/abs/1206.2944},
year = {2012}
}
@article{Jones1998,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
doi = {10.1023/A:1008306431147},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Schonlau, Welch - 1998 - Efficient Global Optimization of Expensive Black-Box Functions.pdf:pdf},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
number = {4},
pages = {455--492},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1008306431147.pdf},
volume = {13},
year = {1998}
}
@article{Laaber2019,
abstract = {Rigorous performance engineering traditionally assumes measuring on bare-metal environments to control for as many confounding factors as possible. Unfortunately, some researchers and practitioners might not have access, knowledge, or funds to operate dedicated performance-testing hardware, making public clouds an attractive alternative. However, shared public cloud environments are inherently unpredictable in terms of the system performance they provide. In this study, we explore the effects of cloud environments on the variability of performance test results and to what extent slowdowns can still be reliably detected even in a public cloud. We focus on software microbenchmarks as an example of performance tests and execute extensive experiments on three different well-known public cloud services (AWS, GCE, and Azure) using three different cloud instance types per service. We also compare the results to a hosted bare-metal offering from IBM Bluemix. In total, we gathered more than 4.5 million unique microbenchmarking data points from benchmarks written in Java and Go. We find that the variability of results differs substantially between benchmarks and instance types (by a coefficient of variation from 0.03{\%} to {\textgreater}{\{}$\backslash$thinspace{\}}100{\%}). However, executing test and control experiments on the same instances (in randomized order) allows us to detect slowdowns of 10{\%} or less with high confidence, using state-of-the-art statistical tests (i.e., Wilcoxon rank-sum and overlapping bootstrapped confidence intervals). Finally, our results indicate that Wilcoxon rank-sum manages to detect smaller slowdowns in cloud environments.},
author = {Laaber, Christoph and Scheuner, Joel and Leitner, Philipp},
doi = {10.1007/s10664-019-09681-1},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laaber, Scheuner, Leitner - 2019 - Software microbenchmarking in the cloud. How bad is it really.pdf:pdf},
issn = {15737616},
journal = {Empirical Software Engineering},
keywords = {Cloud,Microbenchmarking,Performance testing,Performance-regression detection},
month = {apr},
pages = {1--40},
publisher = {Springer US},
title = {{Software microbenchmarking in the cloud. How bad is it really?}},
url = {http://link.springer.com/10.1007/s10664-019-09681-1},
year = {2019}
}
@inproceedings{Varghese2016a,
abstract = {Existing benchmarking methods are time consuming processes as they typically benchmark the entire Virtual Machine (VM) in order to generate accurate performance data, making them less suitable for real-time analytics. The research in this paper is aimed to surmount the above challenge by presenting DocLite - Docker Container-based Lightweight benchmarking tool. DocLite explores lightweight cloud benchmarking methods for rapidly executing benchmarks in near real-time. DocLite is built on the Docker container technology, which allows a user-defined memory size and number of CPU cores of the VM to be benchmarked. The tool incorporates two benchmarking methods - the first referred to as the native method employs containers to benchmark a small portion of the VM and generate performance ranks, and the second uses historic benchmark data along with the native method as a hybrid to generate VM ranks. The proposed methods are evaluated on three use-cases and are observed to be up to 91 times faster than benchmarking the entire VM. In both methods, small containers provide the same quality of rankings as a large container. The native method generates ranks with over 90{\%} and 86{\%} accuracy for sequential and parallel execution of an application compared against benchmarking the whole VM. The hybrid method did not improve the quality of the rankings significantly.},
author = {Varghese, Blesson and Subba, Lawan Thamsuhang and Thai, Long and Barker, Adam},
booktitle = {Proceedings - 2016 16th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2016},
doi = {10.1109/CCGrid.2016.14},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varghese et al. - 2016 - DocLite A Docker-Based Lightweight Cloud Benchmarking Tool.pdf:pdf},
isbn = {9781509024520},
keywords = {Docker,cloud benchmarking,containers,hybrid benchmark,lightweight benchmark},
month = {may},
pages = {213--222},
publisher = {IEEE},
title = {{DocLite: A Docker-Based Lightweight Cloud Benchmarking Tool}},
url = {http://ieeexplore.ieee.org/document/7515691/},
year = {2016}
}
@article{Shahriari2016,
abstract = {—Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., rec-ommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable config-uration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and {De Freitas}, Nando},
doi = {10.1109/JPROC.2015.2494218},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahriari et al. - Unknown - Taking the Human Out of the Loop A Review of Bayesian Optimization.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {decision making,design of experiments,genomic medicine,optimization,response surface methodology,statistical learning},
number = {1},
pages = {148--175},
title = {{Taking the human out of the loop: A review of Bayesian optimization}},
url = {http://www.ibm.com/software/commerce/optimization/},
volume = {104},
year = {2016}
}
@inproceedings{Davatz2017,
abstract = {{\textcopyright} 2017 IEEE. A challenging problem for users of Infrastructure-As-A-Service (IaaS) clouds is selecting cloud providers, regions, and instance types cost-optimally for a given desired service level. Issues such as hardware heterogeneity, contention, and virtual machine (VM) placement can result in considerably differing performance across supposedly equivalent cloud resources. Existing research on cloud benchmarking helps, but often the focus is on providing low-level microbenchmarks (e.g., CPU or network speed), which are hard to map to concrete business metrics of enterprise cloud applications, such as request throughput of a multi-Tier Web application. In this paper, we propose Okta, a general approach for fairly and comprehensively benchmarking the performance and cost of a multi-Tier Web application hosted in an IaaS cloud. We exemplify our approach for a case study based on the two-Tier AcmeAir application, which we evaluate for 11 real-life deployment configurations on Amazon EC2 and Google Compute Engine. Our results show that for this application, choosing compute-optimized instance types in the Web layer and small bursting instances for the database tier leads to the overall most cost-effective deployments. This result held true for both cloud providers. The least cost-effective configuration in our study provides only about 67{\%} of throughput per US dollar spent. Our case study can serve as a blueprint for future industrial or academic application benchmarking projects.},
author = {Davatz, Christian and Inzinger, Christian and Scheuner, Joel and Leitner, Philipp},
booktitle = {Proceedings - 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017},
doi = {10.1109/CCGRID.2017.12},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davatz et al. - 2017 - An Approach and Case Study of Cloud Instance Type Selection for Multi-Tier Web Applications.pdf:pdf},
isbn = {9781509066100},
keywords = {Application benchmarks,Bnchmarking,Cloud computing,Web applications},
month = {may},
pages = {534--543},
publisher = {IEEE},
title = {{An Approach and Case Study of Cloud Instance Type Selection for Multi-Tier Web Applications}},
url = {http://ieeexplore.ieee.org/document/7973740/},
year = {2017}
}
@inproceedings{Ruiz-Alvarez2011,
abstract = {We present a new, automated approach to selecting the cloud storage service that best matches each dataset of a given application. Our approach relies on a machine read- able description of the capabilities (features, performance, cost, etc.) of each storage system, which is processed to- gether with the users specified requirements. The result is an assignment of datasets to storage systems, that has mul- tiple advantages: the resulting match meets performance requirements and estimates cost; users express their stor- age needs using high-level concepts rather than reading the documentation from different cloud providers and manu- ally calculating or estimating a solution. Together with our storage capabilities XML schema we present different use cases for our system that evaluate the Amazon, Azure and local clouds under several scenarios: choosing cloud stor- age services for a new application, estimating cost savings by switching storage services, estimating the evolution over time of cost and performance and providing information in an Amazon EC2 to Eucalyptus migration. Our application is able to process each use case in under 70 ms; it is also possible to easily expand it to account for new features and data requirements.},
address = {San Jose, California, USA},
author = {Ruiz-Alvarez, Arkaitz and Humphrey, Marty},
booktitle = {Proceedings of the 2nd international workshop on Scientific cloud computing - ScienceCloud '11},
doi = {10.1145/1996109.1996117},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruiz-Alvarez, Humphrey - 2011 - An automated approach to cloud storage service selection.pdf:pdf},
isbn = {9781450306997},
keywords = {cloud computing,cloud storage services,matching algorithms},
pages = {39},
publisher = {ACM Press},
title = {{An automated approach to cloud storage service selection}},
url = {http://portal.acm.org/citation.cfm?doid=1996109.1996117},
year = {2011}
}
@article{Chase2017,
abstract = {A key benefit of Amazon EC2-style cloud computing service is the ability to instantiate a large number of virtual machines (VMs) on the fly during flash crowd events. Most existing research focuses on the policy decision such as when and where to start a VM for an application. In this paper, we study a different problem: how can the VMs and the applications inside be brought up as quickly as possible? This problem has not been solved satisfactorily in existing cloud services. We develop a fast start technique for cloud applications by restoring previously created VM snapshots of fully initialized application. We propose a set of optimizations, including working set estimation, demand prediction, and free page avoidance, that allow an application to start running with only partially loaded memory, yet without noticeable performance penalty during its subsequent execution. We implement our system, called Twinkle, in the Xen hypervisor and employ the two-dimensional page walks supported by the latest virtualization technology. We use the RUBiS and TPC-W benchmarks to evaluate its performance under flash crowd and failure over scenarios. The results indicate that Twinkle can provision VMs and restore the QoS significantly faster than the current approaches.},
author = {Chase, Jonathan and Niyato, Dusit},
doi = {10.1109/TSC.2015.2476812},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chase, Niyato - 2017 - Joint Optimization of Resource Provisioning in Cloud Computing.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Cloud computing,scenario tree reduction,sensitivity analysis,software defined networking,stochastic optimization},
month = {apr},
number = {3},
pages = {396--409},
title = {{Joint Optimization of Resource Provisioning in Cloud Computing}},
url = {http://ieeexplore.ieee.org/document/5710870/},
volume = {10},
year = {2017}
}
@inproceedings{Varghese2016,
abstract = {With the availability of a wide range of cloud Virtual Machines (VMs) it is difficult to determine which VMs can maximise the performance of an application. Benchmarking is commonly used to this end for capturing the performance of VMs. Most cloud benchmarking techniques are typically heavyweight - time consuming processes which have to benchmark the entire VM in order to obtain accurate benchmark data. Such benchmarks cannot be used in real-time on the cloud and incur extra costs even before an application is deployed. In this paper, we present lightweight cloud benchmarking techniques that execute quickly and can be used in near real-time on the cloud. The exploration of lightweight benchmarking techniques are facilitated by the development of DocLite - Docker Container-based Lightweight Benchmarking. DocLite is built on the Docker container technology which allows a user-defined portion (such as memory size and the number of CPU cores) of the VM to be benchmarked. DocLite operates in two modes, in the first mode, containers are used to benchmark a small portion of the VM to generate performance ranks. In the second mode, historic benchmark data is used along with the first mode as a hybrid to generate VM ranks. The generated ranks are evaluated against three scientific high-performance computing applications. The proposed techniques are up to 91 times faster than a heavyweight technique which benchmarks the entire VM. It is observed that the first mode can generate ranks with over 90{\%} and 86{\%} accuracy for sequential and parallel execution of an application. The hybrid mode improves the correlation slightly but the first mode is sufficient for benchmarking cloud VMs.},
author = {Varghese, Blesson and Subba, Lawan Thamsuhang and Thai, Long and Barker, Adam},
booktitle = {Proceedings - 2016 IEEE International Conference on Cloud Engineering, IC2E 2016: Co-located with the 1st IEEE International Conference on Internet-of-Things Design and Implementation, IoTDI 2016},
doi = {10.1109/IC2E.2016.28},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varghese et al. - 2016 - Container-based cloud virtual machine benchmarking.pdf:pdf},
isbn = {9781509019618},
keywords = {Docker,cloud benchmarking,containers,hybrid benchmark,lightweight benchmark},
month = {apr},
pages = {192--201},
publisher = {IEEE},
title = {{Container-based cloud virtual machine benchmarking}},
url = {http://ieeexplore.ieee.org/document/7484184/},
year = {2016}
}
@inproceedings{Scheuner2018,
abstract = {The continuing growth of the cloud computing market has led to an unprecedented diversity of cloud services. To support service selection, micro-benchmarks are commonly used to identify the best performing cloud service. However, it remains unclear how relevant these synthetic micro-benchmarks are for gaining insights into the performance of real-world applications. Therefore, this paper develops a cloud benchmarking methodology that uses micro-benchmarks to profile applications and subsequently predicts how an application performs on a wide range of cloud services. A study with a real cloud provider (Amazon EC2) has been conducted to quantitatively evaluate the estimation model with 38 metrics from 23 micro-benchmarks and 2 applications from different domains. The results reveal remarkably low variability in cloud service performance and show that selected micro-benchmarks can estimate the duration of a scientific computing application with a relative error of less than 10{\%} and the response time of a Web serving application with a relative error between 10{\%} and 20{\%}. In conclusion, this paper emphasizes the importance of cloud benchmarking by substantiating the suitability of micro-benchmarks for estimating application performance in comparison to common baselines but also highlights that only selected micro-benchmarks are relevant to estimate the performance of a particular application.},
author = {Scheuner, Joel and Leitner, Philipp},
booktitle = {IEEE International Conference on Cloud Computing, CLOUD},
doi = {10.1109/CLOUD.2018.00019},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheuner, Leitner - 2018 - Estimating Cloud Application Performance Based on Micro-Benchmark Profiling.pdf:pdf},
isbn = {9781538672358},
issn = {21596190},
keywords = {Application benchmark,Benchmarking,Cloud computing,Micro benchmark,Performance,Performance prediction,Web application},
month = {jul},
pages = {90--97},
publisher = {IEEE},
title = {{Estimating Cloud Application Performance Based on Micro-Benchmark Profiling}},
url = {https://ieeexplore.ieee.org/document/8457787/},
volume = {2018-July},
year = {2018}
}
@article{Leitner2014,
abstract = {Benchmarking the performance of public cloud providers is a common research topic. Previous research has already extensively evaluated the performance of different cloud platforms for different use cases, and under different constraints and experiment setups. In this paper, we present a principled, large-scale literature review to collect and codify existing research regarding the predictability of performance in public Infrastructure-as-a-Service (IaaS) clouds. We formulate 15 hypotheses relating to the nature of performance variations in IaaS systems, to the factors of influence of performance variations, and how to compare different instance types. In a second step, we conduct extensive real-life experimentation on Amazon EC2 and Google Compute Engine to empirically validate those hypotheses. At the time of our research, performance in EC2 was substantially less predictable than in GCE. Further, we show that hardware heterogeneity is in practice less prevalent than anticipated by earlier research, while multi-tenancy has a dramatic impact on performance and predictability.},
archivePrefix = {arXiv},
arxivId = {1411.2429},
author = {Leitner, Philipp and Cito, Juergen},
doi = {10.1145/2885497},
eprint = {1411.2429},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leitner, Cito - 2014 - Patterns in the Chaos - a Study of Performance Variation and Predictability in Public IaaS Clouds(2).pdf:pdf},
issn = {15335399},
journal = {ACM Transactions on Internet Technology},
keywords = {Infrastructure-as-a-service,benchmarking,public cloud},
month = {apr},
number = {3},
pages = {1--23},
publisher = {ACM},
title = {{Patterns in the Chaos - a Study of Performance Variation and Predictability in Public IaaS Clouds}},
url = {http://dl.acm.org/citation.cfm?doid=2926746.2885497 http://arxiv.org/abs/1411.2429},
volume = {16},
year = {2014}
}
@inproceedings{Scheuner2018a,
abstract = {Micro and application performance benchmarks are commonly used to guide cloud service selection. However, they are often considered in isolation in a hardly reproducible setup with a flawed execution strategy. This paper presents a new execution methodology that combines micro and application benchmarks into a benchmark suite called RMIT Combined, integrates this suite into an automated cloud benchmarking environment, and implements a repeat-able execution strategy. Additionally, a newly crafted Web serving benchmark called WPBench with three different load scenarios is contributed. A case study in the Amazon EC2 cloud demonstrates that choosing a cost-efficient instance type can deliver up to 40{\%} better performance with 40{\%} lower costs at the same time for the Web serving benchmark WPBench. Contrary to prior research, our findings reveal that network performance does not vary relevantly anymore. Our results also show that choosing a modern type of virtualization can improve disk utilization up to 10{\%} for I/O-heavy workloads. CCS CONCEPTS • Software and its engineering → Cloud computing; Software performance;},
author = {Scheuner, Joel and Leitner, Philipp},
doi = {10.1145/3185768.3186286},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheuner, Leitner - 2018 - A Cloud Benchmark Suite Combining Micro and Applications Benchmarks.pdf:pdf},
pages = {161--166},
title = {{A Cloud Benchmark Suite Combining Micro and Applications Benchmarks}},
year = {2018}
}
@inproceedings{Borhani2014,
abstract = {{\textcopyright} 2014 IEEE.Approaching a comprehensive performance benchmark for on-line transaction processing (OLTP) applications in a cloud environment is a challenging task. Fundamental features of clouds, such as the pay-as-you-go pricing model and unknown underlying configuration of the system, are contrary to the basic assumptions of available benchmarks such as TPC-W or RUBiS. In this paper, we introduce a systematic performance benchmark approach for OLTP applications on public clouds that use virtual machines(VMs). We propose WPress benchmark, which is based on the widespread blogging software, WordPress, as a representative OLTP application and implement an open source workload generator. Furthermore, we utilize a CPU micro-benchmark to investigate CPU performance of cloud-based VMs in greater detail. Average response time and total VM cost are the performance metrics measured by WPress. We evaluate small and large instance types of three real-life cloud providers, Amazon EC2, Microsoft Azure and Rackspace cloud. Results imply that Rackspace cloud has better average response times and total VM cost on small instances. However, Microsoft Azure is preferable for large instance type.},
author = {Borhani, Amir Hossein and Leitner, Philipp and Lee, Bu Sung and Li, Xiaorong and Hung, Terence},
booktitle = {Proceedings . IEEE 18th international Enterprise Distributed object computing conference},
doi = {10.1109/EDOC.2014.23},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Borhani et al. - 2014 - WPress An Application-Driven Performance Benchmark for Cloud-Based Virtual Machines.pdf:pdf},
issn = {15417719},
keywords = {Amazon EC2,Benchmarking,CPU Micro-Benchmark,Cloud Computing,Microsoft Azure,OLTP,Rackspace Cloud,Virtual Machine},
number = {December},
pages = {101--109},
title = {{WPress: An Application-Driven Performance Benchmark for Cloud-Based Virtual Machines}},
volume = {2014-Decem},
year = {2014}
}
@article{Armbrust2009,
abstract = {Cloud Computing, the long-held dream of computing as a utility, has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased. Developers with innovative ideas for new Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about over- provisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or under- provisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hlarge scale, is unprecedented in the history of IT.},
author = {Armbrust, Michael and Fox, Armando and Griffith, Rean and Joseph, Anthony D. and Katz, Randy H. and Konwinski, Andrew and Lee, Gunho and Patterson, David A. and Rabkin, Ariel and Stoica, Ion and Zaharia, Matei},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Armbrust et al. - 2009 - Above the cloud.pdf.pdf:pdf},
journal = {EECS Department, University of California, Berkeley},
month = {feb},
pages = {1--25},
title = {{Above the Clouds: A Berkeley View of Cloud Computing}},
url = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html},
year = {2009}
}
@inproceedings{Agarwal2012,
abstract = {Performant execution of data-parallel jobs needs good execution plans. Certain properties of the code, the data, and the interaction between them are crucial to generate these plans. Yet, these properties are difficult to estimate due to the highly distributed nature of these frameworks, the freedom that allows users to specify arbitrary code as operations on the data, and since jobs in modern clusters have evolved beyond single map and reduce phases to logical graphs of operations. Using fixed apriori estimates of these properties to choose execution plans, as modern systems do, leads to poor performance in several instances. We present RoPE, a first step towards re-optimizing data-parallel jobs. RoPE collects certain code and data properties by piggybacking on job execution. It adapts execution plans by feeding these properties to a query optimizer. We show how this improves the future invocations of the same (and similar) jobs and characterize the scenarios of benefit. Experiments on Bing's production clusters show up to 2× improvement across response time for production jobs at the 75th percentile while using 1.5× fewer resources.},
author = {Agarwal, Sameer and Kandula, Srikanth and Bruno, Nico and Wu, Ming-Chuan and Stoica, Ion and Zhou, Jingren},
booktitle = {NSDI'12 Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation},
file = {:C$\backslash$:/Users/jackb/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal et al. - Unknown - nsdi12-final228.pdf.pdf:pdf},
pages = {281--294},
publisher = {USENIX},
title = {{Re-optimizing data-parallel computing}},
url = {https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final228.pdf},
year = {2012}
}
@Manual{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2018},
    url = {https://www.R-project.org/}
}
@Manual{RStudio,
    title = {RStudio: Integrated Development Environment for R},
    author = {{RStudio Team}},
    organization = {RStudio, Inc.},
    address = {Boston, MA},
    year = {2016},
    url = {http://www.rstudio.com/}
}
